* Big Queryのstream insertについて
[Big Queryのデータ投入方式]
- Big Queryにデータを投入する方法として、基本的には3つの方式が挙げられます。
1. データセットの一括読み込み
2. データを1行1行流して投入していく（Stream insert）
3. クエリを利用して書き込む

[方式それぞれの特徴について]
1. データセットの一括読み込み
- CSVファイルやデータベースからのバルクデータ（比較的大規模データ）を一括で投入することに向いた仕組み
- すでにあるデータやまとまったデータをロードするようなケースで有用。（データの移行やバッチ処理など）
- デメリットとしては、リアルタイムでのデータの同期には向いていないので、投入してからデータが参照できる様になるまでにタイムラグが出てしまう。
参考：https://cloud.google.com/bigquery/docs/loading-data?hl=ja
2. ストリームインサート
- ログの転送やイベント処理など時系列に処理されるデータ（ストリームデータ）を順次投入することに向いた仕組み。
- またBig QueryのStream insertは高速に処理できることもメリットになる。つまり、連携元のデータとのタイムラグを極小化できる。
- デメリットとしては有料機能であり、比較的コストが掛かってしまうこと
参考：https://cloud.google.com/bigquery/streaming-data-into-bigquery?hl=ja
３．クエリを利用して書き込む
- すでにデータがBQに存在する場合は、そのデータを利用して新たにデータを書き込むことができます。
- 外部データCSVや、その他のイベントからのデータ書き込みではなく、Big Queryの中でのデータ加工になります。
- 更新元がBQであれば最も有用な手段かと思います。

[ご質問]
「Voucherが更新される都度、バックエンドでBQへ転送（非同期でOK。5分に1回、10分に1回等のサイクル発動でも良い）。キーが一致するデータが存在する場合はUpdate、存在しない場合はInsertする。」
１．Kindで更新処理(put)が走った後にストリーミングインサートを行う。
２．Cloud FunctionsとCloud Schedulerを用いてBQへインサートする

[質問に対する考察]
- Stream insertを利用するケース
結論としては難しいと思われます。
まず、行いたいことについてはVoucherのデータ更新が走ったときに、そのデータのコピーをBQに作りたいという要件だと思われます。
まずStream insertでは「キーが一致するデータが存在する場合はUpdate、存在しない場合はInsertする。」この要件を満たすのが難しいです。
Stream Insertではベストエフォート型の重複排除により、insertidを指定して重複があった場合に、あとに更新されたほうが有効になります。
ただ、この機能は最大1分間の間の更新のみしか重複排除されません。
つまり、同じキーの更新頻度が一分以内のものしかこの処理ができないことになります。
Voucherのキーが1分以下のみの更新しか起こり得ないという仕様であれば対応できますが、あまり考えにくいです。
参考（https://cloud.google.com/bigquery/streaming-data-into-bigquery?hl=ja#dataconsistency）

- Cloud FunctionsとCloud Schedulerを用いてBQへインサートする
1. の方式と比べ実現性があります。
Cloud functionsで定期感覚でVoucherデータを取得しておき、そのKeyが含まれるかどうかをBQをSelectしてチェックします。
存在すればUpdate、なければUpdateというロジックをCloud functionsを使って実行することになります。
insert/updateの処理はMergeの記載をすることで実行が可能となります。
http://kesin.hatenablog.com/entry/2018/06/18/080000


[別の提案]
1. Insert onlyの処理方法とする
そもそもBigTableがリレーショナルデータベースと比較して、
トランザクションやプライマリーキーを保証しているものではありません。

同じキーに対するアップデートの頻度が多く、そのデータの参照が多いのであれば上記１．２．の方式が良いのですが、
かりに、あまり更新はなく基本はInsert処理がメインとなるのでしたら、
Update処理は行わず、参照するタイミングで重複を除去し最新のデータを参照するという方式もコスト優位になる可能性もあります。

つまり、この方式は以下の方式となります。
1. Kind -> Stream InsertでBQにロードする。
2. BQはキー重複されたデータが保存される
3. 参照時に重複除去するクエリを記載し、最新のデータを参照するようにする。

この方式はGoogleの公式ドキュメントにも記載されており、このクエリのビューを作ればアプリケーション用のQueryへの影響はありません。
https://cloud.google.com/bigquery/streaming-data-into-bigquery?hl=ja#manually_removing_duplicates

メルカリでも同様の処理をしているようです。
https://engineering.mercari.com/blog/entry/2018-06-28-100000/

2. データ同期製品を利用
また、こちらの要件があくまでデータを同期することであれば、
以下のようなツールを購入してデータ同期を実装することも一つの方法となります。
https://www.cloud-ace.jp/cap_solution/cdata-sync/

一つの案としてご検討ください。
